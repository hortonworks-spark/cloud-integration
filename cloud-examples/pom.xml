<project>
  <groupId>com.hortonworks.spark</groupId>
  <artifactId>cloud-examples</artifactId>
  <modelVersion>4.0.0</modelVersion>
  <name>cloud-examples</name>

  <packaging>jar</packaging>
  <version>1.0</version>

<!--

  <repositories>
    <repository>
      <id>Spark Staging Repo</id>
      <url>https://repository.apache.org/content/repositories/orgapachespark-1075/</url>
    </repository>
  </repositories>
-->

  <properties>
    <!-- These examples use the version of Hadoop which Spark was built with-->
    <spark.version>2.2.0-SNAPSHOT</spark.version>
    <scala.binary.version>2.11</scala.binary.version>
    <maven.properties.version>1.0-alpha-2</maven.properties.version>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>

    <java.version>1.8</java.version>
    <scala.binary.version>2.11</scala.binary.version>
    <scala.version>2.11.6</scala.version>
    <scala.version.tools>${scala.version}</scala.version.tools>

    <!-- plugin versions -->
    <maven-exec-plugin.version>1.4.0</maven-exec-plugin.version>
    <maven-surefire-plugin.version>2.19</maven-surefire-plugin.version>
    <!-- org.scala.tools-->
    <maven-scala-plugin.version>2.15.2</maven-scala-plugin.version>
    <!--net.alchim31.maven -->
    <alchim.scala-maven-plugin.version>3.2.2</alchim.scala-maven-plugin.version>
    <scalatest-maven-plugin.version>1.0</scalatest-maven-plugin.version>
    <!-- override point: Hadoop XML file containing (directly or via XInclude references)
    the credentials needed to test against target cloud infrastructures. -->
    <cloud.test.configuration.file>(unset)</cloud.test.configuration.file>
    <log4j.debug>false</log4j.debug>
    <central.repo>https://repo1.maven.org/maven2</central.repo>
    <!-- earlier versions of the SPARK-7481 patch had it as spark-cloud.JAR; this
    variable makes it possible to run these tests against that version. -->
    <spark.cloud.jar>spark-hadoop-cloud</spark.cloud.jar>
  </properties>

  <repositories>
    <repository>
      <id>central</id>
      <!-- This should be at top, it makes maven try the central repo first and then others and hence faster dep resolution -->
      <name>Maven Repository</name>
      <url>${central.repo}</url>
      <releases>
        <enabled>true</enabled>
      </releases>
      <snapshots>
        <enabled>false</enabled>
      </snapshots>
    </repository>
  </repositories>

  <pluginRepositories>
    <pluginRepository>
      <id>central</id>
      <url>${central.repo}</url>
      <releases>
        <enabled>true</enabled>
      </releases>
      <snapshots>
        <enabled>false</enabled>
      </snapshots>
    </pluginRepository>
  </pluginRepositories>

  <dependencies>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-core_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
    </dependency>

    <!--Used for test classes -->
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-core_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
      <type>test-jar</type>
      <scope>test</scope>
    </dependency>


    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-sql_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-streaming_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>${spark.cloud.jar}_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-hive_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-mllib_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
    </dependency>

    <dependency>
      <groupId>org.scalatest</groupId>
      <artifactId>scalatest_${scala.binary.version}</artifactId>
      <version>2.2.1</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.mockito</groupId>
      <artifactId>mockito-core</artifactId>
      <version>1.9.5</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.scalacheck</groupId>
      <artifactId>scalacheck_${scala.binary.version}</artifactId>
      <version>1.11.3</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>4.11</version>
      <scope>test</scope>
    </dependency>

    <dependency>
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-log4j12</artifactId>
      <version>1.7.16</version>
    </dependency>

  </dependencies>
  <build>
    <sourceDirectory>src/main/scala</sourceDirectory>
    <testSourceDirectory>src/test/scala</testSourceDirectory>
    <plugins>
      <!--read in a build.properties file if defined-->
      <plugin>
        <groupId>org.codehaus.mojo</groupId>
        <artifactId>properties-maven-plugin</artifactId>
        <version>${maven.properties.version}</version>
        <executions>
          <execution>
            <phase>initialize</phase>
            <goals>
              <goal>read-project-properties</goal>
            </goals>
            <configuration>
              <quiet>true</quiet>
              <files>
                <file>build.properties</file>
                <file>../build.properties</file>
              </files>
            </configuration>
          </execution>
        </executions>
      </plugin>

<!--
      <plugin>
      <groupId>org.scala-tools</groupId>
      <artifactId>maven-scala-plugin</artifactId>
      <version>${maven-scala-plugin.version}</version>
      <executions>
        <execution>
        <goals>
          <goal>compile</goal>
          <goal>testCompile</goal>
        </goals>
        </execution>
      </executions>
      </plugin>
-->

      <plugin>
        <groupId>net.alchim31.maven</groupId>
        <artifactId>scala-maven-plugin</artifactId>
        <version>${alchim.scala-maven-plugin.version}</version>
        <executions>
          <execution>
            <id>eclipse-add-source</id>
            <goals>
              <goal>add-source</goal>
            </goals>
          </execution>
          <execution>
            <id>scala-compile-first</id>
            <phase>process-resources</phase>
            <goals>
              <goal>compile</goal>
            </goals>
          </execution>
          <execution>
            <id>scala-test-compile-first</id>
            <phase>process-test-resources</phase>
            <goals>
              <goal>testCompile</goal>
            </goals>
          </execution>
          <execution>
            <id>attach-scaladocs</id>
            <phase>verify</phase>
            <goals>
              <goal>doc-jar</goal>
            </goals>
          </execution>
        </executions>

        <configuration>
          <scalaVersion>${scala.version}</scalaVersion>
          <recompileMode>incremental</recompileMode>
          <useZincServer>true</useZincServer>
          <args>
            <arg>-unchecked</arg>
            <arg>-deprecation</arg>
            <arg>-feature</arg>
          </args>
          <jvmArgs>
            <jvmArg>-Xms1024m</jvmArg>
            <jvmArg>-Xmx1024m</jvmArg>
<!--
            <jvmArg>-XX:PermSize=${PermGen}</jvmArg>
            <jvmArg>-XX:MaxPermSize=${MaxPermGen}</jvmArg>
            <jvmArg>-XX:ReservedCodeCacheSize=${CodeCacheSize}</jvmArg>
-->
          </jvmArgs>
          <javacArgs>
            <javacArg>-source</javacArg>
            <javacArg>${java.version}</javacArg>
            <javacArg>-target</javacArg>
            <javacArg>${java.version}</javacArg>
            <javacArg>-Xlint:all,-serial,-path</javacArg>
          </javacArgs>
        </configuration>
      </plugin>

      <!-- Enable surefire and scalatest in all children, in one place: -->
      <!-- keep maven-failsafe-plugin version in sync -->
      <!-- Surefire runs all Java tests -->
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-surefire-plugin</artifactId>
        <version>${maven-surefire-plugin.version}</version>
        <!-- Note config is repeated in scalatest config -->
        <configuration>
            <skipTests>true</skipTests>
          <includes>
            <include>**/Test*.java</include>
            <include>**/*Test.java</include>
            <include>**/*TestCase.java</include>
            <include>**/*Suite.java</include>
          </includes>
          <reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>

          <systemProperties>
            <java.awt.headless>true</java.awt.headless>
            <java.io.tmpdir>${project.build.directory}/tmp</java.io.tmpdir>
            <spark.ui.enabled>false</spark.ui.enabled>
            <spark.ui.showConsoleProgress>false</spark.ui.showConsoleProgress>
            <spark.unsafe.exceptionOnMemoryLeak>true</spark.unsafe.exceptionOnMemoryLeak>
          </systemProperties>
          <failIfNoTests>false</failIfNoTests>
        </configuration>
      </plugin>


      <!-- Scalatest runs all Scala tests -->
      <plugin>
        <groupId>org.scalatest</groupId>
        <artifactId>scalatest-maven-plugin</artifactId>
        <version>${scalatest-maven-plugin.version}</version>
        <!-- Note config is repeated in surefire config -->
        <configuration>
          <reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>
          <junitxml>.</junitxml>
          <filereports>SparkTestSuite.txt</filereports>
          <argLine>-ea</argLine>
          <stderr/>
          <environmentVariables>
          </environmentVariables>
          <systemProperties>
            <!--<log4j.configuration>file:src/test/resources/log4j.properties</log4j.configuration>-->
            <log4j.debug>${log4j.debug}</log4j.debug>
            <java.awt.headless>true</java.awt.headless>
            <java.io.tmpdir>${project.build.directory}/tmp</java.io.tmpdir>
            <spark.testing>1</spark.testing>
            <spark.ui.enabled>false</spark.ui.enabled>
            <spark.ui.showConsoleProgress>false</spark.ui.showConsoleProgress>
            <spark.unsafe.exceptionOnMemoryLeak>true</spark.unsafe.exceptionOnMemoryLeak>
            <!-- optionally set for cloud infrastructure tests -->
            <cloud.test.configuration.file>${cloud.test.configuration.file}</cloud.test.configuration.file>

          </systemProperties>

        </configuration>
        <executions>
          <execution>
            <id>test</id>
            <goals>
              <goal>test</goal>
            </goals>
          </execution>
        </executions>
      </plugin>

      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-failsafe-plugin</artifactId>
        <version>${maven-surefire-plugin.version}</version>
      </plugin>

    </plugins>
  </build>

  <profiles>
    <!--
 This is a profile to enable the use of the ASF snapshot and staging repositories
 during a build. It is useful when testing againt nightly or RC releases of dependencies.
 It MUST NOT be used when building copies of Spark to use in production of for distribution,
 -->
    <profile>
      <id>snapshots-and-staging</id>
      <properties>
        <!-- override point for ASF staging/snapshot repos -->
        <asf.staging>https://repository.apache.org/content/groups/staging/</asf.staging>
        <asf.snapshots>https://repository.apache.org/content/repositories/snapshots/</asf.snapshots>
      </properties>

      <pluginRepositories>
        <pluginRepository>
          <id>ASF Staging</id>
          <url>${asf.staging}</url>
        </pluginRepository>
        <pluginRepository>
          <id>ASF Snapshots</id>
          <url>${asf.snapshots}</url>
          <snapshots>
            <enabled>true</enabled>
          </snapshots>
          <releases>
            <enabled>false</enabled>
          </releases>
        </pluginRepository>

      </pluginRepositories>
      <repositories>
        <repository>
          <id>ASF Staging</id>
          <url>${asf.staging}</url>
        </repository>
        <repository>
          <id>ASF Snapshots</id>
          <url>${asf.snapshots}</url>
          <snapshots>
            <enabled>true</enabled>
          </snapshots>
          <releases>
            <enabled>false</enabled>
          </releases>
        </repository>
      </repositories>
    </profile>

    <profile>
      <id>declare-http-components</id>
      <properties>
        <!--  org.apache.httpcomponents/httpclient-->
        <commons.httpclient.version>4.5.2</commons.httpclient.version>
        <commons.httpcore.version>4.4.4</commons.httpcore.version>
      </properties>
      <dependencies>
        <!--Explicit declaration to force in Spark version into transitive dependencies -->
        <dependency>
          <groupId>org.apache.httpcomponents</groupId>
          <artifactId>httpclient</artifactId>
          <version>${commons.httpclient.version}</version>
        </dependency>
        <!--Explicit declaration to force in Spark version into transitive dependencies -->
        <dependency>
          <groupId>org.apache.httpcomponents</groupId>
          <artifactId>httpcore</artifactId>
          <version>${commons.httpcore.version}</version>
        </dependency>
      </dependencies>
    </profile>

  </profiles>
</project>
