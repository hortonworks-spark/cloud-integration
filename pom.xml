<?xml version="1.0" encoding="UTF-8"?>
<!--
  ~ Licensed to the Apache Software Foundation (ASF) under one or more
  ~ contributor license agreements.  See the NOTICE file distributed with
  ~ this work for additional information regarding copyright ownership.
  ~ The ASF licensesÂ´ this file to You under the Apache License, Version 2.0
  ~ (the "License"); you may not use this file except in compliance with
  ~ the License.  You may obtain a copy of the License at
  ~
  ~    http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  -->
<project xmlns="http://maven.apache.org/POM/4.0.0"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>
  <groupId>com.hortonworks.spark</groupId>
  <name>hortonworks-cloud-integration</name>
  <version>1.0-SNAPSHOT</version>
  <artifactId>hortonworks-cloud-integration</artifactId>
  <packaging>pom</packaging>

  <description>
    Cloud integration for Apache Spark. This extends the built in packaging with tests and other modules.
  </description>
  <url>https://github.com/hortonworks-spark/cloud-integration/</url>
  <licenses>
    <license>
      <name>Apache 2.0 License</name>
      <url>http://www.apache.org/licenses/LICENSE-2.0.html</url>
      <distribution>repo</distribution>
    </license>
  </licenses>

  <modules>
    <module>cloud-examples</module>
    <module>spark-cloud-integration</module>
  </modules>

  <properties>
    <java.version>1.8</java.version>
    <scala.version>2.11.8</scala.version>
    <scala.binary.version>2.11</scala.binary.version>

    <!-- These examples use the version of Hadoop which Spark was built with-->
    <spark.version>2.3.0-SNAPSHOT</spark.version>

    <!-- needed to pick up the new stuff-->
    <hadoop.version>3.0.0-beta1-SNAPSHOT</hadoop.version>

    <scala.version.tools>${scala.version}</scala.version.tools>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>


    <!-- Spark shades its Guava use, so a version compatible with Hadoop
     is all that is needed here -->

    <guava.version>19.0</guava.version>

    <test.build.dir>${project.build.directory}/test-dir</test.build.dir>
    <test.build.data>${test.build.dir}</test.build.data>
    <hadoop.tmp.dir>${project.build.directory}/test</hadoop.tmp.dir>

    <!-- plugin versions -->
    <maven-exec-plugin.version>1.4.0</maven-exec-plugin.version>
    <maven-surefire-plugin.version>2.19</maven-surefire-plugin.version>
    <!-- org.scala.tools-->
    <maven-scala-plugin.version>2.15.2</maven-scala-plugin.version>
    <!--net.alchim31.maven -->
    <alchim.scala-maven-plugin.version>3.2.2</alchim.scala-maven-plugin.version>
    <scalatest-maven-plugin.version>1.0</scalatest-maven-plugin.version>
    <!-- override point: Hadoop XML file containing (directly or via XInclude references)
    the credentials needed to test against target cloud infrastructures. -->
    <unset>unset</unset>
    <cloud.test.configuration.file>${unset}</cloud.test.configuration.file>
    <log4j.debug>false</log4j.debug>
    <central.repo>https://repo1.maven.org/maven2</central.repo>
    <!-- earlier versions of the SPARK-7481 patch had it as spark-cloud.JAR; this
    variable makes it possible to run these tests against that version. -->
    <spark.cloud.module.name>spark-hadoop-cloud</spark.cloud.module.name>
    <spark.cloud.module.type>jar</spark.cloud.module.type>
    <committer>${unset}</committer>

    <!-- set this to require a hadoop version-->
    <required.hadoop.version>${unset}</required.hadoop.version>

    <!-- are scale tests enabled ? -->
    <scale.test.enabled>unset</scale.test.enabled>

    <!-- Size in MB of huge files. -->
    <s3a.scale.test.huge.filesize>unset</s3a.scale.test.huge.filesize>
    <!-- Size in MB of the partion size in huge file uploads. -->
    <s3a.scale.test.huge.partitionsize>unset</s3a.scale.test.huge.partitionsize>
    <!-- Timeout in seconds for scale tests.-->
    <s3a.scale.test.timeout>3600</s3a.scale.test.timeout>
    <!-- are s3guard tests enabled ? -->
    <fs.s3a.metadatastore.impl>unset</fs.s3a.metadatastore.impl>
    <fs.s3a.metadatastore.authoritative>false</fs.s3a.metadatastore.authoritative>

    <s3a.s3guard.test.enabled>false</s3a.s3guard.test.enabled>
    <s3a.s3guard.test.authoritative>false</s3a.s3guard.test.authoritative>

    <s3a.s3guard.test.implementation>local</s3a.s3guard.test.implementation>
    <!-- switch to the inconsistent client -->
    <fs.s3a.s3.client.factory.impl>unset</fs.s3a.s3.client.factory.impl>


    <!-- if this flag is set, the code assumes that s3guard is being used and
    then loads _SUCCESS as JSON-->
    <!-- if this flag is set, the code assumes that s3guard is being used and
    then loads _SUCCESS as JSON-->
    <s3a.committer.test.enabled>false</s3a.committer.test.enabled>

    <fs.s3a.committer.name>unset</fs.s3a.committer.name>


    <!-- plugins -->
    <!-- platform encoding override -->
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>

    <!-- maven plugin versions -->
    <maven-deploy-plugin.version>2.8.1</maven-deploy-plugin.version>
    <maven-site-plugin.version>3.6</maven-site-plugin.version>
    <maven-stylus-skin.version>1.5</maven-stylus-skin.version>
    <maven-antrun-plugin.version>1.7</maven-antrun-plugin.version>
    <maven-assembly-plugin.version>2.4</maven-assembly-plugin.version>
    <maven-dependency-plugin.version>2.10</maven-dependency-plugin.version>
    <maven-enforcer-plugin.version>1.4.1</maven-enforcer-plugin.version>
    <maven-javadoc-plugin.version>2.10.4</maven-javadoc-plugin.version>
    <maven-gpg-plugin.version>1.5</maven-gpg-plugin.version>
    <maven.properties.version>1.0-alpha-2</maven.properties.version>
    <maven-remote-resources-plugin.version>1.5</maven-remote-resources-plugin.version>
    <maven-resources-plugin.version>3.0.1</maven-resources-plugin.version>
    <apache-rat-plugin.version>0.10</apache-rat-plugin.version>
    <wagon-ssh.version>1.0</wagon-ssh.version>
    <maven-clover2-plugin.version>3.3.0</maven-clover2-plugin.version>
    <maven-bundle-plugin.version>2.5.0</maven-bundle-plugin.version>
    <lifecycle-mapping.version>1.0.0</lifecycle-mapping.version>
    <maven-checkstyle-plugin.version>2.17</maven-checkstyle-plugin.version>
    <checkstyle.version>7.5.1</checkstyle.version>
    <dependency-check-maven.version>1.4.3</dependency-check-maven.version>


  </properties>

  <repositories>
    <repository>
      <id>central</id>
      <!-- This must be at top, it makes maven try the central repository first
       and then others and hence faster dependency resolution -->
      <name>Maven Repository</name>
      <url>${central.repo}</url>
      <releases>
        <enabled>true</enabled>
      </releases>
      <snapshots>
        <enabled>false</enabled>
      </snapshots>
    </repository>
  </repositories>

  <pluginRepositories>
    <pluginRepository>
      <id>central</id>
      <url>${central.repo}</url>
      <releases>
        <enabled>true</enabled>
      </releases>
      <snapshots>
        <enabled>false</enabled>
      </snapshots>
    </pluginRepository>
  </pluginRepositories>


  <!--
   The hadoop dependencies are explicitly declared
   to guarantee that the s3a committer code comes in,
   along with the associated changes to the hadoop-mapreduce
   JAR.

   This does force this POM to repeat the same exclusions
   of every Hadoop dependency as in spark-hadoop-cloud.

   -->
  <dependencyManagement>
    <dependencies>

      <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>${hadoop.version}</version>
        <exclusions>
          <exclusion>
            <groupId>asm</groupId>
            <artifactId>asm</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.ow2.asm</groupId>
            <artifactId>asm</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.codehaus.jackson</groupId>
            <artifactId>*</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-core</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-annotations</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.fasterxml.jackson.dataformat</groupId>
            <artifactId>jackson-dataformat-cbor</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.fasterxml.jackson.module</groupId>
            <artifactId>jackson-databind</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.fasterxml.jackson.module</groupId>
            <artifactId>jackson-module-jaxb-annotations</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.fasterxml.jackson.jaxrs</groupId>
            <artifactId>jackson-jaxrs-json-provider</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.jboss.netty</groupId>
            <artifactId>netty</artifactId>
          </exclusion>
          <exclusion>
            <groupId>commons-logging</groupId>
            <artifactId>commons-logging</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-all</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.mortbay.jetty</groupId>
            <artifactId>servlet-api-2.5</artifactId>
          </exclusion>
          <exclusion>
            <groupId>javax.servlet</groupId>
            <artifactId>servlet-api</artifactId>
          </exclusion>
          <exclusion>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.sun.jersey</groupId>
            <artifactId>*</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.sun.jersey.jersey-test-framework</groupId>
            <artifactId>*</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.sun.jersey.contribs</groupId>
            <artifactId>*</artifactId>
          </exclusion>
        </exclusions>
      </dependency>
      <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-aws</artifactId>
        <version>${hadoop.version}</version>
        <exclusions>
          <exclusion>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
          </exclusion>
          <exclusion>
            <groupId>commons-logging</groupId>
            <artifactId>commons-logging</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.codehaus.jackson</groupId>
            <artifactId>jackson-mapper-asl</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.codehaus.jackson</groupId>
            <artifactId>jackson-core-asl</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-core</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.fasterxml.jackson.dataformat</groupId>
            <artifactId>jackson-dataformat-cbor</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-annotations</artifactId>
          </exclusion>
        </exclusions>
      </dependency>

      <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-azure</artifactId>
        <version>${hadoop.version}</version>
        <exclusions>
          <exclusion>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.codehaus.jackson</groupId>
            <artifactId>jackson-mapper-asl</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-core</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-annotations</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.google.guava</groupId>
            <artifactId>guava</artifactId>
          </exclusion>
        </exclusions>
      </dependency>

      <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-openstack</artifactId>
        <version>${hadoop.version}</version>
        <exclusions>
          <exclusion>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
          </exclusion>
          <exclusion>
            <groupId>commons-logging</groupId>
            <artifactId>commons-logging</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-annotations</artifactId>
          </exclusion>
          <exclusion>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
          </exclusion>
          <exclusion>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-all</artifactId>
          </exclusion>
        </exclusions>
      </dependency>

      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-core_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <exclusions>
          <exclusion>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
          </exclusion>
        </exclusions>
      </dependency>

      <!--Used for test classes -->
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-core_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <type>test-jar</type>
        <exclusions>
          <exclusion>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
          </exclusion>
        </exclusions>
      </dependency>

      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-sql_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
      </dependency>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-sql_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <type>test-jar</type>
      </dependency>

      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-catalyst_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <type>test-jar</type>
      </dependency>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-catalyst_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
      </dependency>

      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-streaming_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
      </dependency>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>${spark.cloud.module.name}_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <type>jar</type>
        <exclusions>
          <exclusion>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-aws</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-azure</artifactId>
          </exclusion>
          <exclusion>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-openstack</artifactId>
          </exclusion>
        </exclusions>
      </dependency>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-hive_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
      </dependency>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-hive_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <type>test-jar</type>
      </dependency>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-mllib_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
      </dependency>

      <dependency>
        <groupId>org.scalatest</groupId>
        <artifactId>scalatest_${scala.binary.version}</artifactId>
        <version>2.2.6</version>
      </dependency>
      <dependency>
        <groupId>org.mockito</groupId>
        <artifactId>mockito-core</artifactId>
        <version>1.9.5</version>
      </dependency>
      <dependency>
        <groupId>org.scalacheck</groupId>
        <artifactId>scalacheck_${scala.binary.version}</artifactId>
        <version>1.11.3</version>
      </dependency>
      <dependency>
        <groupId>junit</groupId>
        <artifactId>junit</artifactId>
        <version>4.11</version>
      </dependency>
      <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-log4j12</artifactId>
        <version>1.7.16</version>
      </dependency>
      <dependency>
        <groupId>com.google.guava</groupId>
        <artifactId>guava</artifactId>
        <version>${guava.version}</version>
      </dependency>
    </dependencies>
  </dependencyManagement>

  <build>
    <pluginManagement>
      <plugins>
        <plugin>
          <groupId>org.apache.maven.plugins</groupId>
          <artifactId>maven-dependency-plugin</artifactId>
          <version>${maven-dependency-plugin.version}</version>
        </plugin>
        <plugin>
          <groupId>org.apache.maven.plugins</groupId>
          <artifactId>maven-enforcer-plugin</artifactId>
          <version>${maven-enforcer-plugin.version}</version>
          <configuration>
            <rules>
              <requireMavenVersion>
                <version>[3.3.0,)</version>
              </requireMavenVersion>
              <requireJavaVersion>
                <version>[1.8,)</version>
              </requireJavaVersion>
            </rules>
          </configuration>
        </plugin>
        <plugin>
          <groupId>org.apache.maven.plugins</groupId>
          <artifactId>maven-assembly-plugin</artifactId>
          <version>${maven-assembly-plugin.version}</version>
        </plugin>
        <plugin>
          <groupId>org.apache.maven.plugins</groupId>
          <artifactId>maven-site-plugin</artifactId>
          <version>${maven-site-plugin.version}</version>
          <dependencies>
            <dependency><!-- add support for ssh/scp -->
              <groupId>org.apache.maven.wagon</groupId>
              <artifactId>wagon-ssh</artifactId>
              <version>${wagon-ssh.version}</version>
            </dependency>
          </dependencies>
          <configuration>
            <dependencyLocationsEnabled>false</dependencyLocationsEnabled>
          </configuration>
        </plugin>
        <plugin>
          <groupId>org.apache.maven.plugins</groupId>
          <artifactId>maven-compiler-plugin</artifactId>
          <version>3.6.1</version>
          <configuration>
            <source>${java.version}</source>
            <target>${java.version}</target>
            <useIncrementalCompilation>true</useIncrementalCompilation>
            <encoding>UTF-8</encoding>
            <maxmem>1024m</maxmem>
            <fork>true</fork>
            <compilerArgs>
              <arg>-Xlint:all,-serial,-path</arg>
            </compilerArgs>
          </configuration>
        </plugin>
        <plugin>
          <groupId>org.apache.maven.plugins</groupId>
          <artifactId>maven-project-info-reports-plugin</artifactId>
          <version>2.9</version>
        </plugin>
        <plugin>
          <groupId>net.alchim31.maven</groupId>
          <artifactId>scala-maven-plugin</artifactId>
          <version>${alchim.scala-maven-plugin.version}</version>
          <executions>
            <execution>
              <id>eclipse-add-source</id>
              <goals>
                <goal>add-source</goal>
              </goals>
            </execution>
            <execution>
              <id>scala-compile-first</id>
              <phase>process-resources</phase>
              <goals>
                <goal>compile</goal>
              </goals>
            </execution>
            <execution>
              <id>scala-test-compile-first</id>
              <phase>process-test-resources</phase>
              <goals>
                <goal>testCompile</goal>
              </goals>
            </execution>
          </executions>

          <configuration>
            <scalaVersion>${scala.version}</scalaVersion>
            <recompileMode>incremental</recompileMode>
            <useZincServer>true</useZincServer>
            <args>
              <arg>-unchecked</arg>
              <arg>-deprecation</arg>
              <arg>-feature</arg>
            </args>
            <jvmArgs>
              <jvmArg>-Xms256m</jvmArg>
              <jvmArg>-Xmx1024m</jvmArg>
              <!--
                          <jvmArg>-XX:PermSize=${PermGen}</jvmArg>
                          <jvmArg>-XX:MaxPermSize=${MaxPermGen}</jvmArg>
                          <jvmArg>-XX:ReservedCodeCacheSize=${CodeCacheSize}</jvmArg>
              -->
            </jvmArgs>
            <javacArgs>
              <javacArg>-source</javacArg>
              <javacArg>${java.version}</javacArg>
              <javacArg>-target</javacArg>
              <javacArg>${java.version}</javacArg>
              <javacArg>-Xlint:all,-serial,-path</javacArg>
            </javacArgs>
          </configuration>
        </plugin>

        <!-- Scalatest runs all Scala tests -->
        <plugin>
          <groupId>org.scalatest</groupId>
          <artifactId>scalatest-maven-plugin</artifactId>
          <version>${scalatest-maven-plugin.version}</version>
          <!-- Note config is repeated in surefire config -->
          <configuration>
            <reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>
            <junitxml>.</junitxml>
            <filereports>SparkTestSuite.txt</filereports>
            <argLine>-ea</argLine>
            <stderr/>
            <environmentVariables>
            </environmentVariables>
            <systemProperties>
              <!--<log4j.configuration>file:src/test/resources/log4j.properties</log4j.configuration>-->
              <log4j.debug>${log4j.debug}</log4j.debug>
              <java.awt.headless>true</java.awt.headless>
              <java.io.tmpdir>${project.build.directory}/tmp</java.io.tmpdir>
              <test.build.data>${test.build.data}</test.build.data>
              <test.build.dir>${test.build.dir}</test.build.dir>
              <hadoop.tmp.dir>${hadoop.tmp.dir}</hadoop.tmp.dir>
              <spark.testing>1</spark.testing>
              <spark.ui.enabled>false</spark.ui.enabled>
              <spark.ui.showConsoleProgress>false</spark.ui.showConsoleProgress>
              <spark.unsafe.exceptionOnMemoryLeak>true</spark.unsafe.exceptionOnMemoryLeak>
              <cloud.test.configuration.file>${cloud.test.configuration.file}</cloud.test.configuration.file>
              <required.hadoop.version>${required.hadoop.version}</required.hadoop.version>

              <s3a.committer.test.enabled>${s3a.committer.test.enabled}</s3a.committer.test.enabled>
              <fs.s3a.committer.name>${fs.s3a.committer.name}</fs.s3a.committer.name>

              <fs.s3a.s3.client.factory.impl>${fs.s3a.s3.client.factory.impl}</fs.s3a.s3.client.factory.impl>

              <s3a.s3guard.test.enabled>${s3a.s3guard.test.enabled}</s3a.s3guard.test.enabled>
              <s3a.s3guard.test.authoritative>${s3a.s3guard.test.authoritative}</s3a.s3guard.test.authoritative>
              <s3a.s3guard.test.implementation>${s3a.s3guard.test.implementation}</s3a.s3guard.test.implementation>

              <fs.s3a.metadatastore.authoritative>${fs.s3a.metadatastore.authoritative}</fs.s3a.metadatastore.authoritative>
              <fs.s3a.metadatastore.impl>${fs.s3a.metadatastore.impl}</fs.s3a.metadatastore.impl>
            </systemProperties>
          </configuration>
          <executions>
            <execution>
              <id>test</id>
              <goals>
                <goal>test</goal>
              </goals>
            </execution>
          </executions>
        </plugin>

        <plugin>
          <groupId>org.apache.maven.plugins</groupId>
          <artifactId>maven-failsafe-plugin</artifactId>
          <version>${maven-surefire-plugin.version}</version>
        </plugin>
      </plugins>
    </pluginManagement>

    <plugins>
<!--
      <plugin>
        <groupId>net.alchim31.maven</groupId>
        <artifactId>scala-maven-plugin</artifactId>
      </plugin>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-source-plugin</artifactId>
      </plugin>
-->
    </plugins>
  </build>
  <profiles>
    <!--
 This is a profile to enable the use of the ASF snapshot and staging repositories
 during a build. It is useful when testing againt nightly or RC releases of dependencies.
 It MUST NOT be used when building copies of Spark to use in production of for distribution,
 -->
    <profile>
      <id>snapshots-and-staging</id>
      <properties>
        <!-- override point for ASF staging/snapshot repos -->
        <asf.staging>https://repository.apache.org/content/groups/staging/</asf.staging>
        <asf.snapshots>https://repository.apache.org/content/repositories/snapshots/</asf.snapshots>
      </properties>

      <pluginRepositories>
        <pluginRepository>
          <id>ASF Staging</id>
          <url>${asf.staging}</url>
        </pluginRepository>
        <pluginRepository>
          <id>ASF Snapshots</id>
          <url>${asf.snapshots}</url>
          <snapshots>
            <enabled>true</enabled>
          </snapshots>
          <releases>
            <enabled>false</enabled>
          </releases>
        </pluginRepository>

      </pluginRepositories>
      <repositories>
        <repository>
          <id>ASF Staging</id>
          <url>${asf.staging}</url>
        </repository>
        <repository>
          <id>ASF Snapshots</id>
          <url>${asf.snapshots}</url>
          <snapshots>
            <enabled>true</enabled>
          </snapshots>
          <releases>
            <enabled>false</enabled>
          </releases>
        </repository>
      </repositories>
    </profile>

    <profile>
      <id>oldjar</id>
      <properties>
        <spark.cloud.module.name>spark-cloud</spark.cloud.module.name>
      </properties>
    </profile>
<!--

    <profile>
      <id>s3a-tests</id>
      <properties>
        <cloud.test.configuration.file>
          ../../cloud-test-configs/s3a.xml
        </cloud.test.configuration.file>
      </properties>
    </profile>
-->

    <profile>
      <id>hadoop-2.8</id>
      <properties>
        <required.hadoop.version>2.8.0</required.hadoop.version>
      </properties>
    </profile>

    <profile>
      <id>branch-2</id>
      <properties>
        <required.hadoop.version>2.9.0</required.hadoop.version>
      </properties>
    </profile>

    <profile>
      <id>hadoop-3.0</id>
      <properties>
        <!--<required.hadoop.version>3.0.0-alpha4-SNAPSHOT</required.hadoop.version>-->
        <required.hadoop.version>2.11</required.hadoop.version>
        <guava.version>21.0</guava.version>
      </properties>
    </profile>


    <profile>
      <id>declare-http-components</id>
      <dependencies>
        <!--Explicit declaration to force in Spark version into transitive dependencies -->
        <dependency>
          <groupId>org.apache.httpcomponents</groupId>
          <artifactId>httpclient</artifactId>
          <version>4.5.2</version>
        </dependency>
        <!--Explicit declaration to force in Spark version into transitive dependencies -->
        <dependency>
          <groupId>org.apache.httpcomponents</groupId>
          <artifactId>httpcore</artifactId>
          <version>4.4.4</version>
        </dependency>
      </dependencies>
    </profile>

    <profile>
      <id>scale</id>
      <properties>
        <scale.test.enabled>true</scale.test.enabled>
      </properties>
    </profile>

    <profile>
      <id>staging</id>
      <properties>
        <required.hadoop.version>2.11</required.hadoop.version>
        <fs.s3a.s3guard.test.enabled>true</fs.s3a.s3guard.test.enabled>
        <fs.s3a.committer.enabled>true</fs.s3a.committer.enabled>
        <fs.s3a.committer.name>staging</fs.s3a.committer.name>

        <fs.s3a.committer.test.enabled>true</fs.s3a.committer.test.enabled>
      </properties>
    </profile>


    <profile>
      <id>directory</id>
      <properties>
        <required.hadoop.version>2.11</required.hadoop.version>
        <fs.s3a.s3guard.test.enabled>true</fs.s3a.s3guard.test.enabled>
        <fs.s3a.committer.enabled>true</fs.s3a.committer.enabled>
        <fs.s3a.committer.test.enabled>true</fs.s3a.committer.test.enabled>
        <fs.s3a.committer.name>directory</fs.s3a.committer.name>
      </properties>
    </profile>

    <profile>
      <id>partition</id>
      <properties>
        <fs.s3a.s3guard.test.enabled>true</fs.s3a.s3guard.test.enabled>
        <fs.s3a.committer.enabled>true</fs.s3a.committer.enabled>
        <fs.s3a.committer.test.enabled>true</fs.s3a.committer.test.enabled>
        <required.hadoop.version>2.11</required.hadoop.version>
        <fs.s3a.committer.name>partitioned</fs.s3a.committer.name>
      </properties>
    </profile>

    <profile>
      <id>magic</id>
      <properties>
        <fs.s3a.s3guard.test.enabled>true</fs.s3a.s3guard.test.enabled>
        <fs.s3a.committer.enabled>true</fs.s3a.committer.enabled>

        <s3a.committer.test.enabled>true</s3a.committer.test.enabled>
        <required.hadoop.version>2.11</required.hadoop.version>
        <fs.s3a.committer.name>magic</fs.s3a.committer.name>
      </properties>
    </profile>


    <!-- Switch to dynamo DB for s3guard. Has no effect unless s3guard is enabled -->
    <profile>
      <id>dynamo</id>
      <activation>
        <property>
          <name>dynamo</name>
        </property>
      </activation>
      <properties>
        <fs.s3a.metadatastore.impl>org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore</fs.s3a.metadatastore.impl>
        <s3a.s3guard.test.enabled>true</s3a.s3guard.test.enabled>
        <s3a.s3guard.test.implementation>dynamo</s3a.s3guard.test.implementation>
      </properties>
    </profile>

    <!-- Enable S3guard against a DynamoDBLocal datastore -->
    <profile>
      <id>localdynamo</id>
      <activation>
        <property>
          <name>localdynamo</name>
        </property>
      </activation>
      <properties>
        <fs.s3a.metadatastore.impl>org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore</fs.s3a.metadatastore.impl>
        <s3a.s3guard.test.enabled>true</s3a.s3guard.test.enabled>
        <s3a.s3guard.test.implementation>dynamodblocal</s3a.s3guard.test.implementation>
      </properties>
    </profile>

    <!-- Switch s3guard from Authoritative=false to true
     Has no effect unless s3guard is enabled -->
    <profile>
      <id>authoritative</id>
      <activation>
        <property>
          <name>authoritative</name>
        </property>
      </activation>
      <properties>
        <fs.s3a.metadatastore.authoritative>true</fs.s3a.metadatastore.authoritative>
      </properties>
    </profile>


    <!-- inconsistent S3A client -->
    <profile>
      <id>inconsistent</id>
      <activation>
        <property>
          <name>inconsistent</name>
        </property>
      </activation>
      <properties>
        <fs.s3a.s3.client.factory.impl>org.apache.hadoop.fs.s3a.InconsistentS3ClientFactory</fs.s3a.s3.client.factory.impl>
      </properties>
    </profile>

    <!-- failing S3A client -->
    <profile>
      <id>failing</id>
      <activation>
        <property>
          <name>failing</name>
        </property>
      </activation>
      <properties>
        <fs.s3a.s3.client.factory.impl>com.hortonworks.spark.cloud.s3.FailingS3ClientFactory</fs.s3a.s3.client.factory.impl>
      </properties>
    </profile>

  </profiles>

</project>  
